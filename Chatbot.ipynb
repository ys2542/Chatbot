{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "import collections\n",
    "from Seq2Seq import seq2seq_model\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "lines = open('movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "conv_lines = open('movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to map each line's id with its text\n",
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]\n",
    "        \n",
    "# Create a list of all of the conversations' lines' ids.\n",
    "convs = [ ]\n",
    "for line in conv_lines[: -1]:\n",
    "    _line = line.split(' +++$+++ ')[-1][1: -1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    convs.append(_line.split(','))\n",
    "    \n",
    "# Sort the sentences into questions (inputs) and answers (targets)\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for conv in convs:\n",
    "    for i in range(len(conv)-1):\n",
    "        questions.append(id2line[conv[i]])\n",
    "        answers.append(id2line[conv[i + 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "clean_questions = [clean_text(question) for question in questions]\n",
    "clean_answers = [clean_text(answer) for answer in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Remove questions and answers that are shorter than 2 words and longer than 20 words.\n",
    "min_line_length = 2\n",
    "max_line_length = 20\n",
    "\n",
    "# Filter out the questions that are too short/long\n",
    "short_questions_temp = []\n",
    "short_answers_temp = []\n",
    "\n",
    "i = 0\n",
    "for question in clean_questions:\n",
    "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
    "        short_questions_temp.append(question)\n",
    "        short_answers_temp.append(clean_answers[i])\n",
    "    i += 1\n",
    "\n",
    "# Filter out the answers that are too short/long\n",
    "short_questions = []\n",
    "short_answers = []\n",
    "\n",
    "i = 0\n",
    "for answer in short_answers_temp:\n",
    "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
    "        short_answers.append(answer)\n",
    "        short_questions.append(short_questions_temp[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of questions: 138335\n",
      "# of answers: 138335\n",
      "% of data used: 62.419999999999995%\n"
     ]
    }
   ],
   "source": [
    "# Compare the number of lines we will use with the total number of lines.\n",
    "print(\"# of questions:\", len(short_questions))\n",
    "print(\"# of answers:\", len(short_answers))\n",
    "print(\"% of data used: {}%\".format(round(len(short_questions)/len(questions),4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary for the frequency of the vocabulary\n",
    "def count_freq(vocab, sentences):\n",
    "    for sentence in sentences:\n",
    "        for word in sentence.split():\n",
    "            vocab[word] += 1\n",
    "    return\n",
    "\n",
    "vocab = collections.defaultdict(int)\n",
    "count_freq(vocab, short_questions)\n",
    "count_freq(vocab, short_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of total vocab: 45618\n",
      "Size of vocab we will use: 8092\n"
     ]
    }
   ],
   "source": [
    "# Remove rare words from the vocabulary.\n",
    "# We will aim to replace fewer than 5% of words with <UNK>\n",
    "# You will see this ratio soon.\n",
    "threshold = 10\n",
    "count = 0\n",
    "for k,v in vocab.items():\n",
    "    if v >= threshold:\n",
    "        count += 1\n",
    "\n",
    "print(\"Size of total vocab:\", len(vocab))\n",
    "print(\"Size of vocab we will use:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# In case we want to use a different vocabulary sizes for the source and target text, \n",
    "# we can set different threshold values.\n",
    "# Nonetheless, we will create dictionaries to provide a unique integer for each word.\n",
    "questions_vocab_to_int = {}\n",
    "answers_vocab_to_int = {}\n",
    "\n",
    "def normal_tokenize(threshold, vocab, vocab_to_int):\n",
    "    start_number = 0\n",
    "    for word, count in vocab.items():\n",
    "        if count >= threshold:\n",
    "            vocab_to_int[word] = start_number\n",
    "            start_number += 1\n",
    "            \n",
    "def add_special_tokens(vocab_to_int, start_number):\n",
    "    vocab_to_int['<PAD>'] = start_number\n",
    "    vocab_to_int['<EOS>'] = start_number + 1\n",
    "    vocab_to_int['<UNK>'] = start_number + 2\n",
    "    vocab_to_int['<GO>'] = start_number + 3\n",
    "\n",
    "normal_tokenize(threshold, vocab, questions_vocab_to_int)\n",
    "normal_tokenize(threshold, vocab, answers_vocab_to_int)\n",
    "\n",
    "add_special_tokens(questions_vocab_to_int, len(questions_vocab_to_int))\n",
    "add_special_tokens(answers_vocab_to_int, len(answers_vocab_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no i ' m not <EOS> <EOS>\n"
     ]
    }
   ],
   "source": [
    "# Create dictionaries to map the unique integers to their respective words.\n",
    "# i.e. an inverse dictionary for vocab_to_int.\n",
    "questions_int_to_vocab = {v_i: v for v, v_i in questions_vocab_to_int.items()}\n",
    "answers_int_to_vocab = {v_i: v for v, v_i in answers_vocab_to_int.items()}\n",
    "\n",
    "# Add the end of sentence token to the end of every answer.\n",
    "for i in range(len(short_answers)):\n",
    "    short_answers[i] += ' <EOS>'\n",
    "\n",
    "print (short_answers[111])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert the text to integers. Replace any words that are not in the respective vocabulary with <UNK> \n",
    "questions_int = []\n",
    "for question in short_questions:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in questions_vocab_to_int:\n",
    "            ints.append(questions_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(questions_vocab_to_int[word])\n",
    "    questions_int.append(ints)\n",
    "    \n",
    "answers_int = []\n",
    "for answer in short_answers:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in answers_vocab_to_int:\n",
    "            ints.append(answers_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(answers_vocab_to_int[word])\n",
    "    answers_int.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of words:  2334533\n",
      "Number of times <UNK> is used:  92436\n",
      "ratio of words that are <UNK>:  0.03959507104847094\n"
     ]
    }
   ],
   "source": [
    "# Calculate what percentage of all words have been replaced with <UNK>\n",
    "def count_unk(sentence_int, unk_int):\n",
    "    word_count = 0\n",
    "    unk_count = 0\n",
    "    for sentence in sentence_int:\n",
    "        for w in sentence:\n",
    "            if w == unk_int:\n",
    "                unk_count += 1\n",
    "            word_count += 1\n",
    "    return word_count, unk_count\n",
    "\n",
    "question_word_count, question_unk_count = count_unk(questions_int, questions_vocab_to_int['<UNK>'])\n",
    "answer_word_count, answer_unk_count = count_unk(answers_int, answers_vocab_to_int['<UNK>'])\n",
    "word_count = question_word_count + answer_word_count\n",
    "unk_count = question_unk_count + answer_unk_count\n",
    "\n",
    "unk_ratio = unk_count / word_count\n",
    "\n",
    "print('total number of words: ', word_count)\n",
    "print('Number of times <UNK> is used: ', unk_count)\n",
    "print('ratio of words that are <UNK>: ', '{}'.format(unk_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117585\n",
      "20750\n"
     ]
    }
   ],
   "source": [
    "# Sort questions and answers by the length of questions\n",
    "# (This will reduce the amount of padding during training, which should speed up training and help to reduce the loss\n",
    "sorted_questions = []\n",
    "sorted_answers = []\n",
    "\n",
    "for length in range(1, max_line_length + 1):\n",
    "    for i in enumerate(questions_int):\n",
    "        if len(i[1]) == length:\n",
    "            sorted_questions.append(questions_int[i[0]])\n",
    "            sorted_answers.append(answers_int[i[0]])\n",
    "\n",
    "# Train_test split\n",
    "train_test_split = int(len(sorted_questions) * 0.15)\n",
    "\n",
    "train_questions = sorted_questions[train_test_split:]\n",
    "train_answers = sorted_answers[train_test_split:]\n",
    "\n",
    "test_questions = sorted_questions[:train_test_split]\n",
    "test_answers = sorted_answers[:train_test_split]\n",
    "\n",
    "print(len(train_questions))\n",
    "print(len(test_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    # Create palceholders for inputs to the model\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob\n",
    "\n",
    "def pad_sentence_batch(sentence_batch, vocab_to_int):\n",
    "    # Pad sentences with <PAD> \n",
    "    max_sentence_len = 0\n",
    "    for sentence in sentence_batch:\n",
    "        max_sentence_len = max(max_sentence_len, len(sentence))\n",
    "    return [sentence + ([vocab_to_int['<PAD>']] * (max_sentence_len - len(sentence))) for sentence in sentence_batch]\n",
    "\n",
    "def batch_data(questions, answers, batch_size):\n",
    "    # Iterator to batch questions and answers\n",
    "    for batch_i in range(0, len(questions) // batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        questions_batch = questions[start_i : start_i + batch_size]\n",
    "        answers_batch = answers[start_i : start_i + batch_size]\n",
    "        pad_questions_batch = np.array(pad_sentence_batch(questions_batch, questions_vocab_to_int))\n",
    "        pad_answers_batch = np.array(pad_sentence_batch(answers_batch, answers_vocab_to_int))\n",
    "        yield pad_questions_batch, pad_answers_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 3\n",
    "encoding_embedding_size = 256\n",
    "decoding_embedding_size = 256\n",
    "learning_rate = 0.1\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.01\n",
    "keep_probability = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Reset the graph to ensure that it is ready for training\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "    \n",
    "# Load the model inputs    \n",
    "input_data, targets, lr, keep_prob = model_inputs()\n",
    "# Sequence length will be the max line length for each batch\n",
    "sequence_length = tf.placeholder_with_default(max_line_length, None, name = 'sequence_length')\n",
    "# Find the shape of the input data for sequence_loss\n",
    "input_shape = tf.shape(input_data)\n",
    "\n",
    "# Create the training and inference logits\n",
    "train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]), targets, keep_prob, batch_size, sequence_length, \n",
    "                                               len(answers_vocab_to_int), len(questions_vocab_to_int), encoding_embedding_size, \n",
    "                                               decoding_embedding_size, rnn_size, num_layers, questions_vocab_to_int)\n",
    "\n",
    "# Create a tensor for the inference logits, needed if loading a checkpoint version of the model\n",
    "tf.identity(inference_logits, 'logits')\n",
    "\n",
    "with tf.name_scope(\"optimization\"):\n",
    "    seq_loss = tf.contrib.seq2seq.sequence_loss(train_logits, targets, tf.ones([input_shape[0], sequence_length]))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients = optimizer.compute_gradients(seq_loss)\n",
    "    # Gradient Clipping\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# If you're loading a pre-train model (ref_logits), DO NOT RUN THE BLCOK BELOW!!! \n",
    "# Run this block and you'll load the ref_logits \n",
    "#checkpoint = \"./ckpts/best_model.ckpt\" \n",
    "#saver = tf.train.Saver() \n",
    "#saver.restore(sess, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/50 Batch    0/1837 - Loss:  0.030, Seconds: 323.34\n",
      "Epoch   1/50 Batch  300/1837 - Loss: 64.806, Seconds: 36.35\n",
      "Epoch   1/50 Batch  600/1837 - Loss: 91.210, Seconds: 37.59\n",
      "Epoch   1/50 Batch  900/1837 - Loss: 93.497, Seconds: 39.42\n",
      "Valid Loss: 46.235, Seconds: 11.92\n",
      "Not Fully Initiated\n",
      "Epoch   1/50 Batch 1200/1837 - Loss: 76.653, Seconds: 41.92\n",
      "Epoch   1/50 Batch 1500/1837 - Loss: 93.026, Seconds: 43.87\n",
      "Epoch   1/50 Batch 1800/1837 - Loss: 94.422, Seconds: 50.25\n",
      "Valid Loss: 242.490, Seconds: 11.88\n",
      "New Record!\n",
      "Epoch   2/50 Batch    0/1837 - Loss: 30.001, Seconds: 33.71\n",
      "Epoch   2/50 Batch  300/1837 - Loss: 103.049, Seconds: 37.46\n",
      "Epoch   2/50 Batch  600/1837 - Loss: 78.963, Seconds: 37.98\n",
      "Epoch   2/50 Batch  900/1837 - Loss: 80.814, Seconds: 39.45\n",
      "Valid Loss: 112.021, Seconds: 11.87\n",
      "New Record!\n",
      "Epoch   2/50 Batch 1200/1837 - Loss: 78.490, Seconds: 41.30\n",
      "Epoch   2/50 Batch 1500/1837 - Loss: 76.052, Seconds: 44.14\n",
      "Epoch   2/50 Batch 1800/1837 - Loss: 79.187, Seconds: 49.42\n",
      "Valid Loss: 56.486, Seconds: 11.86\n",
      "New Record!\n",
      "Epoch   3/50 Batch    0/1837 - Loss:  8.768, Seconds: 33.63\n",
      "Epoch   3/50 Batch  300/1837 - Loss: 144.902, Seconds: 36.94\n",
      "Epoch   3/50 Batch  600/1837 - Loss: 86.088, Seconds: 37.73\n",
      "Epoch   3/50 Batch  900/1837 - Loss: 64.656, Seconds: 39.80\n",
      "Valid Loss: 39.710, Seconds: 11.89\n",
      "New Record!\n",
      "Epoch   3/50 Batch 1200/1837 - Loss: 126.779, Seconds: 41.29\n",
      "Epoch   3/50 Batch 1500/1837 - Loss: 88.006, Seconds: 44.90\n",
      "Epoch   3/50 Batch 1800/1837 - Loss: 121.585, Seconds: 49.21\n",
      "Valid Loss: 146.935, Seconds: 11.90\n",
      "No Improvement.\n",
      "Epoch   4/50 Batch    0/1837 - Loss: 13.179, Seconds: 33.56\n",
      "Epoch   4/50 Batch  300/1837 - Loss: 105.814, Seconds: 36.82\n",
      "Epoch   4/50 Batch  600/1837 - Loss: 102.034, Seconds: 37.93\n",
      "Epoch   4/50 Batch  900/1837 - Loss: 92.379, Seconds: 40.34\n",
      "Valid Loss: 62.225, Seconds: 11.88\n",
      "No Improvement.\n",
      "Epoch   4/50 Batch 1200/1837 - Loss: 121.937, Seconds: 41.27\n",
      "Epoch   4/50 Batch 1500/1837 - Loss: 98.368, Seconds: 44.66\n",
      "Epoch   4/50 Batch 1800/1837 - Loss: 88.639, Seconds: 48.89\n",
      "Valid Loss: 84.088, Seconds: 11.87\n",
      "No Improvement.\n",
      "Epoch   5/50 Batch    0/1837 - Loss: 15.915, Seconds: 33.52\n",
      "Epoch   5/50 Batch  300/1837 - Loss: 118.285, Seconds: 36.68\n",
      "Epoch   5/50 Batch  600/1837 - Loss: 113.646, Seconds: 37.38\n",
      "Epoch   5/50 Batch  900/1837 - Loss: 80.316, Seconds: 40.19\n",
      "Valid Loss: 61.814, Seconds: 11.88\n",
      "No Improvement.\n",
      "Epoch   5/50 Batch 1200/1837 - Loss: 95.259, Seconds: 41.28\n",
      "Epoch   5/50 Batch 1500/1837 - Loss: 102.941, Seconds: 44.17\n",
      "Epoch   5/50 Batch 1800/1837 - Loss: 122.603, Seconds: 48.90\n",
      "Valid Loss: 147.981, Seconds: 11.88\n",
      "No Improvement.\n",
      "Epoch   6/50 Batch    0/1837 - Loss: 10.581, Seconds: 33.64\n",
      "Epoch   6/50 Batch  300/1837 - Loss: 116.044, Seconds: 36.77\n",
      "Epoch   6/50 Batch  600/1837 - Loss: 125.393, Seconds: 37.58\n",
      "Epoch   6/50 Batch  900/1837 - Loss: 117.589, Seconds: 40.24\n",
      "Valid Loss: 90.923, Seconds: 11.87\n",
      "No Improvement.\n",
      "Epoch   6/50 Batch 1200/1837 - Loss: 96.655, Seconds: 41.04\n",
      "Epoch   6/50 Batch 1500/1837 - Loss: 87.176, Seconds: 45.08\n",
      "Epoch   6/50 Batch 1800/1837 - Loss: 105.452, Seconds: 49.35\n",
      "Valid Loss: 149.520, Seconds: 11.88\n",
      "No Improvement.\n",
      "Epoch   7/50 Batch    0/1837 - Loss: 17.254, Seconds: 33.76\n",
      "Epoch   7/50 Batch  300/1837 - Loss: 109.801, Seconds: 36.75\n",
      "Epoch   7/50 Batch  600/1837 - Loss: 99.645, Seconds: 37.47\n",
      "Epoch   7/50 Batch  900/1837 - Loss: 76.132, Seconds: 40.17\n",
      "Valid Loss: 99.762, Seconds: 11.87\n",
      "No Improvement.\n",
      "Epoch   7/50 Batch 1200/1837 - Loss: 90.523, Seconds: 41.32\n",
      "Epoch   7/50 Batch 1500/1837 - Loss: 76.024, Seconds: 44.33\n",
      "Epoch   7/50 Batch 1800/1837 - Loss: 125.229, Seconds: 48.79\n",
      "Valid Loss: 175.090, Seconds: 11.91\n",
      "No Improvement.\n",
      "Epoch   8/50 Batch    0/1837 - Loss: 15.661, Seconds: 33.64\n",
      "Epoch   8/50 Batch  300/1837 - Loss: 121.065, Seconds: 36.75\n",
      "Epoch   8/50 Batch  600/1837 - Loss: 83.209, Seconds: 37.33\n",
      "Epoch   8/50 Batch  900/1837 - Loss: 107.305, Seconds: 40.14\n",
      "Valid Loss: 131.876, Seconds: 11.88\n",
      "No Improvement.\n",
      "Epoch   8/50 Batch 1200/1837 - Loss: 113.951, Seconds: 41.07\n",
      "Epoch   8/50 Batch 1500/1837 - Loss: 88.291, Seconds: 45.70\n",
      "Epoch   8/50 Batch 1800/1837 - Loss: 144.897, Seconds: 49.22\n",
      "Valid Loss: 114.773, Seconds: 11.86\n",
      "No Improvement.\n",
      "Epoch   9/50 Batch    0/1837 - Loss: 12.884, Seconds: 33.63\n",
      "Epoch   9/50 Batch  300/1837 - Loss: 110.323, Seconds: 36.94\n",
      "Epoch   9/50 Batch  600/1837 - Loss: 107.184, Seconds: 37.44\n",
      "Epoch   9/50 Batch  900/1837 - Loss: 106.112, Seconds: 40.48\n",
      "Valid Loss: 61.759, Seconds: 11.87\n",
      "No Improvement.\n",
      "Epoch   9/50 Batch 1200/1837 - Loss: 99.489, Seconds: 41.03\n",
      "Epoch   9/50 Batch 1500/1837 - Loss: 73.345, Seconds: 45.93\n",
      "Epoch   9/50 Batch 1800/1837 - Loss: 102.918, Seconds: 49.07\n",
      "Valid Loss: 112.512, Seconds: 11.87\n",
      "No Improvement.\n",
      "Epoch  10/50 Batch    0/1837 - Loss: 17.534, Seconds: 33.56\n",
      "Epoch  10/50 Batch  300/1837 - Loss: 124.325, Seconds: 37.25\n",
      "Epoch  10/50 Batch  600/1837 - Loss: 98.512, Seconds: 37.73\n",
      "Epoch  10/50 Batch  900/1837 - Loss: 127.547, Seconds: 40.37\n",
      "Valid Loss: 59.342, Seconds: 11.88\n",
      "No Improvement.\n",
      "Epoch  10/50 Batch 1200/1837 - Loss: 71.502, Seconds: 41.01\n",
      "Epoch  10/50 Batch 1500/1837 - Loss: 73.196, Seconds: 45.92\n",
      "Epoch  10/50 Batch 1800/1837 - Loss: 115.452, Seconds: 49.12\n",
      "Valid Loss: 242.658, Seconds: 11.87\n",
      "No Improvement.\n",
      "Epoch  11/50 Batch    0/1837 - Loss: 14.498, Seconds: 33.63\n",
      "Epoch  11/50 Batch  300/1837 - Loss: 138.986, Seconds: 37.19\n",
      "Epoch  11/50 Batch  600/1837 - Loss: 97.302, Seconds: 37.55\n",
      "Epoch  11/50 Batch  900/1837 - Loss: 92.087, Seconds: 39.51\n",
      "Valid Loss: 79.435, Seconds: 11.86\n",
      "No Improvement.\n",
      "Epoch  11/50 Batch 1200/1837 - Loss: 77.224, Seconds: 41.18\n",
      "Epoch  11/50 Batch 1500/1837 - Loss: 130.829, Seconds: 43.87\n",
      "Epoch  11/50 Batch 1800/1837 - Loss: 120.406, Seconds: 49.34\n",
      "Valid Loss: 116.504, Seconds: 11.87\n",
      "No Improvement.\n",
      "Epoch  12/50 Batch    0/1837 - Loss: 10.457, Seconds: 33.45\n",
      "Epoch  12/50 Batch  300/1837 - Loss: 136.935, Seconds: 37.35\n",
      "Epoch  12/50 Batch  600/1837 - Loss: 116.027, Seconds: 37.91\n",
      "Epoch  12/50 Batch  900/1837 - Loss: 86.177, Seconds: 39.56\n",
      "Valid Loss: 119.174, Seconds: 11.87\n",
      "No Improvement.\n",
      "Epoch  12/50 Batch 1200/1837 - Loss: 85.078, Seconds: 41.29\n",
      "Epoch  12/50 Batch 1500/1837 - Loss: 112.307, Seconds: 43.77\n",
      "Epoch  12/50 Batch 1800/1837 - Loss: 101.350, Seconds: 49.34\n",
      "Valid Loss: 173.129, Seconds: 11.87\n",
      "No Improvement.\n",
      "Epoch  13/50 Batch    0/1837 - Loss: 25.608, Seconds: 33.28\n",
      "Epoch  13/50 Batch  300/1837 - Loss: 93.298, Seconds: 38.11\n",
      "Epoch  13/50 Batch  600/1837 - Loss: 101.728, Seconds: 37.86\n",
      "Epoch  13/50 Batch  900/1837 - Loss: 91.581, Seconds: 39.54\n",
      "Valid Loss: 285.149, Seconds: 11.87\n",
      "No Improvement.\n",
      "Epoch  13/50 Batch 1200/1837 - Loss: 100.898, Seconds: 41.51\n",
      "Epoch  13/50 Batch 1500/1837 - Loss: 96.146, Seconds: 43.96\n",
      "Epoch  13/50 Batch 1800/1837 - Loss: 180.956, Seconds: 50.28\n",
      "Valid Loss: 139.351, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  14/50 Batch    0/1837 - Loss: 14.539, Seconds: 33.52\n",
      "Epoch  14/50 Batch  300/1837 - Loss: 112.089, Seconds: 36.80\n",
      "Epoch  14/50 Batch  600/1837 - Loss: 90.392, Seconds: 37.91\n",
      "Epoch  14/50 Batch  900/1837 - Loss: 128.009, Seconds: 39.65\n",
      "Valid Loss: 198.999, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  14/50 Batch 1200/1837 - Loss: 107.060, Seconds: 41.61\n",
      "Epoch  14/50 Batch 1500/1837 - Loss: 120.801, Seconds: 44.08\n",
      "Epoch  14/50 Batch 1800/1837 - Loss: 139.288, Seconds: 49.31\n",
      "Valid Loss: 124.448, Seconds: 11.88\n",
      "No Improvement.\n",
      "Epoch  15/50 Batch    0/1837 - Loss: 21.027, Seconds: 33.15\n",
      "Epoch  15/50 Batch  300/1837 - Loss: 143.726, Seconds: 36.92\n",
      "Epoch  15/50 Batch  600/1837 - Loss: 125.469, Seconds: 37.61\n",
      "Epoch  15/50 Batch  900/1837 - Loss: 102.797, Seconds: 40.12\n",
      "Valid Loss: 52.897, Seconds: 11.88\n",
      "No Improvement.\n",
      "Epoch  15/50 Batch 1200/1837 - Loss: 72.154, Seconds: 41.70\n",
      "Epoch  15/50 Batch 1500/1837 - Loss: 111.225, Seconds: 43.82\n",
      "Epoch  15/50 Batch 1800/1837 - Loss: 110.461, Seconds: 50.17\n",
      "Valid Loss: 79.072, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  16/50 Batch    0/1837 - Loss: 11.532, Seconds: 34.38\n",
      "Epoch  16/50 Batch  300/1837 - Loss: 126.866, Seconds: 36.56\n",
      "Epoch  16/50 Batch  600/1837 - Loss: 104.734, Seconds: 37.72\n",
      "Epoch  16/50 Batch  900/1837 - Loss: 99.947, Seconds: 39.58\n",
      "Valid Loss: 168.324, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  16/50 Batch 1200/1837 - Loss: 82.466, Seconds: 41.26\n",
      "Epoch  16/50 Batch 1500/1837 - Loss: 110.765, Seconds: 44.80\n",
      "Epoch  16/50 Batch 1800/1837 - Loss: 93.478, Seconds: 49.38\n",
      "Valid Loss: 71.657, Seconds: 11.90\n",
      "No Improvement.\n",
      "Epoch  17/50 Batch    0/1837 - Loss: 11.023, Seconds: 33.68\n",
      "Epoch  17/50 Batch  300/1837 - Loss: 101.070, Seconds: 37.16\n",
      "Epoch  17/50 Batch  600/1837 - Loss: 161.615, Seconds: 37.86\n",
      "Epoch  17/50 Batch  900/1837 - Loss: 139.234, Seconds: 39.60\n",
      "Valid Loss: 83.536, Seconds: 11.88\n",
      "No Improvement.\n",
      "Epoch  17/50 Batch 1200/1837 - Loss: 91.882, Seconds: 41.68\n",
      "Epoch  17/50 Batch 1500/1837 - Loss: 93.681, Seconds: 44.10\n",
      "Epoch  17/50 Batch 1800/1837 - Loss: 117.553, Seconds: 51.04\n",
      "Valid Loss: 117.398, Seconds: 11.88\n",
      "No Improvement.\n",
      "Epoch  18/50 Batch    0/1837 - Loss: 13.956, Seconds: 34.16\n",
      "Epoch  18/50 Batch  300/1837 - Loss: 145.401, Seconds: 37.00\n",
      "Epoch  18/50 Batch  600/1837 - Loss: 138.190, Seconds: 38.39\n",
      "Epoch  18/50 Batch  900/1837 - Loss: 92.447, Seconds: 39.50\n",
      "Valid Loss: 86.493, Seconds: 11.88\n",
      "No Improvement.\n",
      "Epoch  18/50 Batch 1200/1837 - Loss: 111.508, Seconds: 41.28\n",
      "Epoch  18/50 Batch 1500/1837 - Loss: 93.796, Seconds: 44.23\n",
      "Epoch  18/50 Batch 1800/1837 - Loss: 128.342, Seconds: 49.43\n",
      "Valid Loss: 159.893, Seconds: 11.91\n",
      "No Improvement.\n",
      "Epoch  19/50 Batch    0/1837 - Loss: 13.292, Seconds: 33.96\n",
      "Epoch  19/50 Batch  300/1837 - Loss: 124.399, Seconds: 36.96\n",
      "Epoch  19/50 Batch  600/1837 - Loss: 113.287, Seconds: 37.87\n",
      "Epoch  19/50 Batch  900/1837 - Loss: 90.910, Seconds: 40.19\n",
      "Valid Loss: 77.438, Seconds: 11.94\n",
      "No Improvement.\n",
      "Epoch  19/50 Batch 1200/1837 - Loss: 103.209, Seconds: 41.37\n",
      "Epoch  19/50 Batch 1500/1837 - Loss: 88.580, Seconds: 44.18\n",
      "Epoch  19/50 Batch 1800/1837 - Loss: 137.336, Seconds: 49.91\n",
      "Valid Loss: 199.859, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  20/50 Batch    0/1837 - Loss: 22.115, Seconds: 33.78\n",
      "Epoch  20/50 Batch  300/1837 - Loss: 132.344, Seconds: 36.76\n",
      "Epoch  20/50 Batch  600/1837 - Loss: 124.104, Seconds: 38.36\n",
      "Epoch  20/50 Batch  900/1837 - Loss: 90.743, Seconds: 39.44\n",
      "Valid Loss: 81.485, Seconds: 11.86\n",
      "No Improvement.\n",
      "Epoch  20/50 Batch 1200/1837 - Loss: 151.950, Seconds: 41.34\n",
      "Epoch  20/50 Batch 1500/1837 - Loss: 108.123, Seconds: 44.51\n",
      "Epoch  20/50 Batch 1800/1837 - Loss: 90.979, Seconds: 48.87\n",
      "Valid Loss: 125.392, Seconds: 11.87\n",
      "No Improvement.\n",
      "Epoch  21/50 Batch    0/1837 - Loss: 24.583, Seconds: 33.62\n",
      "Epoch  21/50 Batch  300/1837 - Loss: 133.724, Seconds: 36.88\n",
      "Epoch  21/50 Batch  600/1837 - Loss: 98.537, Seconds: 37.48\n",
      "Epoch  21/50 Batch  900/1837 - Loss: 98.666, Seconds: 39.91\n",
      "Valid Loss: 102.449, Seconds: 11.88\n",
      "No Improvement.\n",
      "Epoch  21/50 Batch 1200/1837 - Loss: 107.887, Seconds: 41.34\n",
      "Epoch  21/50 Batch 1500/1837 - Loss: 89.567, Seconds: 44.02\n",
      "Epoch  21/50 Batch 1800/1837 - Loss: 127.573, Seconds: 50.35\n",
      "Valid Loss: 125.632, Seconds: 11.87\n",
      "No Improvement.\n",
      "Epoch  22/50 Batch    0/1837 - Loss: 11.275, Seconds: 33.92\n",
      "Epoch  22/50 Batch  300/1837 - Loss: 106.285, Seconds: 36.75\n",
      "Epoch  22/50 Batch  600/1837 - Loss: 137.080, Seconds: 38.41\n",
      "Epoch  22/50 Batch  900/1837 - Loss: 94.225, Seconds: 39.40\n",
      "Valid Loss: 105.303, Seconds: 11.86\n",
      "No Improvement.\n",
      "Epoch  22/50 Batch 1200/1837 - Loss: 102.245, Seconds: 41.28\n",
      "Epoch  22/50 Batch 1500/1837 - Loss: 113.571, Seconds: 44.49\n",
      "Epoch  22/50 Batch 1800/1837 - Loss: 102.234, Seconds: 48.99\n",
      "Valid Loss: 553.412, Seconds: 11.91\n",
      "No Improvement.\n",
      "Epoch  23/50 Batch    0/1837 - Loss: 34.548, Seconds: 33.61\n",
      "Epoch  23/50 Batch  300/1837 - Loss: 125.351, Seconds: 36.97\n",
      "Epoch  23/50 Batch  600/1837 - Loss: 121.189, Seconds: 37.30\n",
      "Epoch  23/50 Batch  900/1837 - Loss: 124.285, Seconds: 39.91\n",
      "Valid Loss: 245.945, Seconds: 12.02\n",
      "No Improvement.\n",
      "Epoch  23/50 Batch 1200/1837 - Loss: 109.585, Seconds: 41.34\n",
      "Epoch  23/50 Batch 1500/1837 - Loss: 97.866, Seconds: 43.95\n",
      "Epoch  23/50 Batch 1800/1837 - Loss: 130.180, Seconds: 49.72\n",
      "Valid Loss: 225.760, Seconds: 11.87\n",
      "No Improvement.\n",
      "Epoch  24/50 Batch    0/1837 - Loss: 32.053, Seconds: 33.72\n",
      "Epoch  24/50 Batch  300/1837 - Loss: 129.460, Seconds: 36.77\n",
      "Epoch  24/50 Batch  600/1837 - Loss: 104.108, Seconds: 38.98\n",
      "Epoch  24/50 Batch  900/1837 - Loss: 118.381, Seconds: 39.62\n",
      "Valid Loss: 95.335, Seconds: 11.96\n",
      "No Improvement.\n",
      "Epoch  24/50 Batch 1200/1837 - Loss: 88.031, Seconds: 41.40\n",
      "Epoch  24/50 Batch 1500/1837 - Loss: 122.473, Seconds: 44.50\n",
      "Epoch  24/50 Batch 1800/1837 - Loss: 126.925, Seconds: 49.05\n",
      "Valid Loss: 154.647, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  25/50 Batch    0/1837 - Loss: 21.689, Seconds: 33.50\n",
      "Epoch  25/50 Batch  300/1837 - Loss: 118.437, Seconds: 36.89\n",
      "Epoch  25/50 Batch  600/1837 - Loss: 118.940, Seconds: 37.66\n",
      "Epoch  25/50 Batch  900/1837 - Loss: 128.278, Seconds: 39.52\n",
      "Valid Loss: 422.655, Seconds: 11.88\n",
      "No Improvement.\n",
      "Epoch  25/50 Batch 1200/1837 - Loss: 128.882, Seconds: 41.23\n",
      "Epoch  25/50 Batch 1500/1837 - Loss: 114.436, Seconds: 44.43\n",
      "Epoch  25/50 Batch 1800/1837 - Loss: 87.668, Seconds: 49.93\n",
      "Valid Loss: 282.117, Seconds: 11.88\n",
      "No Improvement.\n",
      "Epoch  26/50 Batch    0/1837 - Loss: 12.656, Seconds: 33.64\n",
      "Epoch  26/50 Batch  300/1837 - Loss: 128.589, Seconds: 36.78\n",
      "Epoch  26/50 Batch  600/1837 - Loss: 119.807, Seconds: 37.78\n",
      "Epoch  26/50 Batch  900/1837 - Loss: 93.618, Seconds: 39.52\n",
      "Valid Loss: 93.116, Seconds: 11.90\n",
      "No Improvement.\n",
      "Epoch  26/50 Batch 1200/1837 - Loss: 102.869, Seconds: 41.32\n",
      "Epoch  26/50 Batch 1500/1837 - Loss: 100.200, Seconds: 44.09\n",
      "Epoch  26/50 Batch 1800/1837 - Loss: 106.614, Seconds: 51.25\n",
      "Valid Loss: 120.909, Seconds: 11.88\n",
      "No Improvement.\n",
      "Epoch  27/50 Batch    0/1837 - Loss: 13.864, Seconds: 34.45\n",
      "Epoch  27/50 Batch  300/1837 - Loss: 139.733, Seconds: 36.82\n",
      "Epoch  27/50 Batch  600/1837 - Loss: 131.432, Seconds: 38.74\n",
      "Epoch  27/50 Batch  900/1837 - Loss: 109.597, Seconds: 39.32\n",
      "Valid Loss: 89.585, Seconds: 11.87\n",
      "No Improvement.\n",
      "Epoch  27/50 Batch 1200/1837 - Loss: 101.311, Seconds: 42.09\n",
      "Epoch  27/50 Batch 1500/1837 - Loss: 113.222, Seconds: 43.99\n",
      "Epoch  27/50 Batch 1800/1837 - Loss: 91.144, Seconds: 49.41\n",
      "Valid Loss: 121.616, Seconds: 11.88\n",
      "No Improvement.\n",
      "Epoch  28/50 Batch    0/1837 - Loss: 12.855, Seconds: 33.92\n",
      "Epoch  28/50 Batch  300/1837 - Loss: 106.186, Seconds: 37.32\n",
      "Epoch  28/50 Batch  600/1837 - Loss: 154.104, Seconds: 37.79\n",
      "Epoch  28/50 Batch  900/1837 - Loss: 81.073, Seconds: 40.46\n",
      "Valid Loss: 93.373, Seconds: 11.90\n",
      "No Improvement.\n",
      "Epoch  28/50 Batch 1200/1837 - Loss: 110.517, Seconds: 42.37\n",
      "Epoch  28/50 Batch 1500/1837 - Loss: 125.766, Seconds: 44.53\n",
      "Epoch  28/50 Batch 1800/1837 - Loss: 149.754, Seconds: 49.05\n",
      "Valid Loss: 225.530, Seconds: 11.87\n",
      "No Improvement.\n",
      "Epoch  29/50 Batch    0/1837 - Loss: 35.522, Seconds: 33.77\n",
      "Epoch  29/50 Batch  300/1837 - Loss: 104.615, Seconds: 37.54\n",
      "Epoch  29/50 Batch  600/1837 - Loss: 126.589, Seconds: 38.02\n",
      "Epoch  29/50 Batch  900/1837 - Loss: 137.328, Seconds: 39.46\n",
      "Valid Loss: 157.949, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  29/50 Batch 1200/1837 - Loss: 124.147, Seconds: 41.71\n",
      "Epoch  29/50 Batch 1500/1837 - Loss: 101.380, Seconds: 44.16\n",
      "Epoch  29/50 Batch 1800/1837 - Loss: 130.151, Seconds: 49.66\n",
      "Valid Loss: 255.189, Seconds: 11.90\n",
      "No Improvement.\n",
      "Epoch  30/50 Batch    0/1837 - Loss: 31.238, Seconds: 33.77\n",
      "Epoch  30/50 Batch  300/1837 - Loss: 98.037, Seconds: 37.14\n",
      "Epoch  30/50 Batch  600/1837 - Loss: 95.884, Seconds: 37.66\n",
      "Epoch  30/50 Batch  900/1837 - Loss: 85.845, Seconds: 39.53\n",
      "Valid Loss: 209.428, Seconds: 11.87\n",
      "No Improvement.\n",
      "Epoch  30/50 Batch 1200/1837 - Loss: 100.838, Seconds: 41.54\n",
      "Epoch  30/50 Batch 1500/1837 - Loss: 93.274, Seconds: 44.05\n",
      "Epoch  30/50 Batch 1800/1837 - Loss: 153.015, Seconds: 49.73\n",
      "Valid Loss: 141.568, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  31/50 Batch    0/1837 - Loss: 11.643, Seconds: 33.51\n",
      "Epoch  31/50 Batch  300/1837 - Loss: 125.594, Seconds: 37.14\n",
      "Epoch  31/50 Batch  600/1837 - Loss: 108.346, Seconds: 37.82\n",
      "Epoch  31/50 Batch  900/1837 - Loss: 105.859, Seconds: 40.06\n",
      "Valid Loss: 111.438, Seconds: 11.88\n",
      "No Improvement.\n",
      "Epoch  31/50 Batch 1200/1837 - Loss: 95.232, Seconds: 41.58\n",
      "Epoch  31/50 Batch 1500/1837 - Loss: 91.408, Seconds: 44.04\n",
      "Epoch  31/50 Batch 1800/1837 - Loss: 127.493, Seconds: 49.32\n",
      "Valid Loss: 176.825, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  32/50 Batch    0/1837 - Loss: 18.593, Seconds: 33.71\n",
      "Epoch  32/50 Batch  300/1837 - Loss: 171.153, Seconds: 37.36\n",
      "Epoch  32/50 Batch  600/1837 - Loss: 109.038, Seconds: 37.84\n",
      "Epoch  32/50 Batch  900/1837 - Loss: 119.736, Seconds: 39.63\n",
      "Valid Loss: 238.668, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  32/50 Batch 1200/1837 - Loss: 124.584, Seconds: 41.36\n",
      "Epoch  32/50 Batch 1500/1837 - Loss: 103.863, Seconds: 44.25\n",
      "Epoch  32/50 Batch 1800/1837 - Loss: 134.647, Seconds: 49.52\n",
      "Valid Loss: 281.326, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  33/50 Batch    0/1837 - Loss: 18.376, Seconds: 33.73\n",
      "Epoch  33/50 Batch  300/1837 - Loss: 112.638, Seconds: 37.57\n",
      "Epoch  33/50 Batch  600/1837 - Loss: 119.982, Seconds: 37.88\n",
      "Epoch  33/50 Batch  900/1837 - Loss: 93.489, Seconds: 39.52\n",
      "Valid Loss: 206.798, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  33/50 Batch 1200/1837 - Loss: 124.874, Seconds: 41.46\n",
      "Epoch  33/50 Batch 1500/1837 - Loss: 87.871, Seconds: 44.16\n",
      "Epoch  33/50 Batch 1800/1837 - Loss: 113.304, Seconds: 49.40\n",
      "Valid Loss: 246.566, Seconds: 11.93\n",
      "No Improvement.\n",
      "Epoch  34/50 Batch    0/1837 - Loss: 20.423, Seconds: 34.55\n",
      "Epoch  34/50 Batch  300/1837 - Loss: 124.537, Seconds: 37.84\n",
      "Epoch  34/50 Batch  600/1837 - Loss: 126.434, Seconds: 38.05\n",
      "Epoch  34/50 Batch  900/1837 - Loss: 92.962, Seconds: 39.32\n",
      "Valid Loss: 359.936, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  34/50 Batch 1200/1837 - Loss: 104.707, Seconds: 41.10\n",
      "Epoch  34/50 Batch 1500/1837 - Loss: 96.895, Seconds: 44.86\n",
      "Epoch  34/50 Batch 1800/1837 - Loss: 123.193, Seconds: 49.15\n",
      "Valid Loss: 199.332, Seconds: 11.88\n",
      "No Improvement.\n",
      "Epoch  35/50 Batch    0/1837 - Loss: 16.341, Seconds: 34.22\n",
      "Epoch  35/50 Batch  300/1837 - Loss: 173.164, Seconds: 36.88\n",
      "Epoch  35/50 Batch  600/1837 - Loss: 129.270, Seconds: 37.78\n",
      "Epoch  35/50 Batch  900/1837 - Loss: 109.801, Seconds: 39.94\n",
      "Valid Loss: 153.570, Seconds: 11.90\n",
      "No Improvement.\n",
      "Epoch  35/50 Batch 1200/1837 - Loss: 107.922, Seconds: 41.35\n",
      "Epoch  35/50 Batch 1500/1837 - Loss: 99.306, Seconds: 43.76\n",
      "Epoch  35/50 Batch 1800/1837 - Loss: 156.715, Seconds: 50.07\n",
      "Valid Loss: 148.852, Seconds: 11.91\n",
      "No Improvement.\n",
      "Epoch  36/50 Batch    0/1837 - Loss: 17.555, Seconds: 33.69\n",
      "Epoch  36/50 Batch  300/1837 - Loss: 123.603, Seconds: 36.72\n",
      "Epoch  36/50 Batch  600/1837 - Loss: 102.891, Seconds: 37.74\n",
      "Epoch  36/50 Batch  900/1837 - Loss: 83.700, Seconds: 40.64\n",
      "Valid Loss: 160.863, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  36/50 Batch 1200/1837 - Loss: 114.566, Seconds: 42.03\n",
      "Epoch  36/50 Batch 1500/1837 - Loss: 94.726, Seconds: 44.33\n",
      "Epoch  36/50 Batch 1800/1837 - Loss: 105.991, Seconds: 49.31\n",
      "Valid Loss: 224.742, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  37/50 Batch    0/1837 - Loss: 20.818, Seconds: 35.07\n",
      "Epoch  37/50 Batch  300/1837 - Loss: 132.982, Seconds: 37.00\n",
      "Epoch  37/50 Batch  600/1837 - Loss: 106.374, Seconds: 38.00\n",
      "Epoch  37/50 Batch  900/1837 - Loss: 116.011, Seconds: 39.63\n",
      "Valid Loss: 195.163, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  37/50 Batch 1200/1837 - Loss: 105.512, Seconds: 41.42\n",
      "Epoch  37/50 Batch 1500/1837 - Loss: 113.633, Seconds: 44.58\n",
      "Epoch  37/50 Batch 1800/1837 - Loss: 117.196, Seconds: 49.71\n",
      "Valid Loss: 169.769, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  38/50 Batch    0/1837 - Loss: 11.574, Seconds: 33.58\n",
      "Epoch  38/50 Batch  300/1837 - Loss: 149.150, Seconds: 36.97\n",
      "Epoch  38/50 Batch  600/1837 - Loss: 109.567, Seconds: 37.71\n",
      "Epoch  38/50 Batch  900/1837 - Loss: 102.878, Seconds: 40.32\n",
      "Valid Loss: 165.512, Seconds: 11.90\n",
      "No Improvement.\n",
      "Epoch  38/50 Batch 1200/1837 - Loss: 138.455, Seconds: 42.50\n",
      "Epoch  38/50 Batch 1500/1837 - Loss: 101.590, Seconds: 44.30\n",
      "Epoch  38/50 Batch 1800/1837 - Loss: 134.394, Seconds: 49.55\n",
      "Valid Loss: 277.668, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  39/50 Batch    0/1837 - Loss: 27.722, Seconds: 33.50\n",
      "Epoch  39/50 Batch  300/1837 - Loss: 128.454, Seconds: 37.08\n",
      "Epoch  39/50 Batch  600/1837 - Loss: 86.222, Seconds: 38.36\n",
      "Epoch  39/50 Batch  900/1837 - Loss: 94.533, Seconds: 39.53\n",
      "Valid Loss: 147.740, Seconds: 11.90\n",
      "No Improvement.\n",
      "Epoch  39/50 Batch 1200/1837 - Loss: 101.241, Seconds: 41.40\n",
      "Epoch  39/50 Batch 1500/1837 - Loss: 96.409, Seconds: 43.78\n",
      "Epoch  39/50 Batch 1800/1837 - Loss: 107.430, Seconds: 50.26\n",
      "Valid Loss: 191.202, Seconds: 11.87\n",
      "No Improvement.\n",
      "Epoch  40/50 Batch    0/1837 - Loss: 29.480, Seconds: 33.80\n",
      "Epoch  40/50 Batch  300/1837 - Loss: 129.999, Seconds: 37.08\n",
      "Epoch  40/50 Batch  600/1837 - Loss: 95.374, Seconds: 37.97\n",
      "Epoch  40/50 Batch  900/1837 - Loss: 110.778, Seconds: 39.55\n",
      "Valid Loss: 227.675, Seconds: 11.87\n",
      "No Improvement.\n",
      "Epoch  40/50 Batch 1200/1837 - Loss: 127.598, Seconds: 41.19\n",
      "Epoch  40/50 Batch 1500/1837 - Loss: 89.156, Seconds: 45.03\n",
      "Epoch  40/50 Batch 1800/1837 - Loss: 136.909, Seconds: 49.32\n",
      "Valid Loss: 146.454, Seconds: 11.94\n",
      "No Improvement.\n",
      "Epoch  41/50 Batch    0/1837 - Loss: 11.218, Seconds: 34.03\n",
      "Epoch  41/50 Batch  300/1837 - Loss: 144.623, Seconds: 36.77\n",
      "Epoch  41/50 Batch  600/1837 - Loss: 135.813, Seconds: 37.76\n",
      "Epoch  41/50 Batch  900/1837 - Loss: 90.752, Seconds: 39.62\n",
      "Valid Loss: 206.717, Seconds: 11.94\n",
      "No Improvement.\n",
      "Epoch  41/50 Batch 1200/1837 - Loss: 109.671, Seconds: 41.36\n",
      "Epoch  41/50 Batch 1500/1837 - Loss: 102.006, Seconds: 44.15\n",
      "Epoch  41/50 Batch 1800/1837 - Loss: 132.667, Seconds: 49.43\n",
      "Valid Loss: 177.211, Seconds: 11.91\n",
      "No Improvement.\n",
      "Epoch  42/50 Batch    0/1837 - Loss: 14.662, Seconds: 33.97\n",
      "Epoch  42/50 Batch  300/1837 - Loss: 135.787, Seconds: 37.22\n",
      "Epoch  42/50 Batch  600/1837 - Loss: 108.371, Seconds: 38.22\n",
      "Epoch  42/50 Batch  900/1837 - Loss: 92.855, Seconds: 39.55\n",
      "Valid Loss: 398.184, Seconds: 11.88\n",
      "No Improvement.\n",
      "Epoch  42/50 Batch 1200/1837 - Loss: 130.801, Seconds: 41.96\n",
      "Epoch  42/50 Batch 1500/1837 - Loss: 98.626, Seconds: 43.80\n",
      "Epoch  42/50 Batch 1800/1837 - Loss: 148.098, Seconds: 49.40\n",
      "Valid Loss: 208.012, Seconds: 11.88\n",
      "No Improvement.\n",
      "Epoch  43/50 Batch    0/1837 - Loss: 24.927, Seconds: 33.44\n",
      "Epoch  43/50 Batch  300/1837 - Loss: 117.277, Seconds: 36.82\n",
      "Epoch  43/50 Batch  600/1837 - Loss: 104.754, Seconds: 38.06\n",
      "Epoch  43/50 Batch  900/1837 - Loss: 101.715, Seconds: 39.53\n",
      "Valid Loss: 178.074, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  43/50 Batch 1200/1837 - Loss: 111.745, Seconds: 41.23\n",
      "Epoch  43/50 Batch 1500/1837 - Loss: 89.469, Seconds: 44.24\n",
      "Epoch  43/50 Batch 1800/1837 - Loss: 135.146, Seconds: 50.47\n",
      "Valid Loss: 240.333, Seconds: 11.90\n",
      "No Improvement.\n",
      "Epoch  44/50 Batch    0/1837 - Loss: 17.493, Seconds: 33.49\n",
      "Epoch  44/50 Batch  300/1837 - Loss: 150.052, Seconds: 36.92\n",
      "Epoch  44/50 Batch  600/1837 - Loss: 142.299, Seconds: 37.76\n",
      "Epoch  44/50 Batch  900/1837 - Loss: 117.132, Seconds: 39.76\n",
      "Valid Loss: 197.941, Seconds: 11.90\n",
      "No Improvement.\n",
      "Epoch  44/50 Batch 1200/1837 - Loss: 87.439, Seconds: 41.29\n",
      "Epoch  44/50 Batch 1500/1837 - Loss: 118.889, Seconds: 44.80\n",
      "Epoch  44/50 Batch 1800/1837 - Loss: 116.335, Seconds: 50.43\n",
      "Valid Loss: 238.462, Seconds: 11.90\n",
      "No Improvement.\n",
      "Epoch  45/50 Batch    0/1837 - Loss: 10.403, Seconds: 34.14\n",
      "Epoch  45/50 Batch  300/1837 - Loss: 161.540, Seconds: 37.61\n",
      "Epoch  45/50 Batch  600/1837 - Loss: 127.374, Seconds: 37.75\n",
      "Epoch  45/50 Batch  900/1837 - Loss: 122.996, Seconds: 40.99\n",
      "Valid Loss: 187.127, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  45/50 Batch 1200/1837 - Loss: 127.656, Seconds: 41.98\n",
      "Epoch  45/50 Batch 1500/1837 - Loss: 99.170, Seconds: 43.90\n",
      "Epoch  45/50 Batch 1800/1837 - Loss: 93.446, Seconds: 50.01\n",
      "Valid Loss: 171.316, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  46/50 Batch    0/1837 - Loss: 10.353, Seconds: 34.32\n",
      "Epoch  46/50 Batch  300/1837 - Loss: 132.932, Seconds: 36.80\n",
      "Epoch  46/50 Batch  600/1837 - Loss: 122.321, Seconds: 38.10\n",
      "Epoch  46/50 Batch  900/1837 - Loss: 99.566, Seconds: 39.97\n",
      "Valid Loss: 175.814, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  46/50 Batch 1200/1837 - Loss: 103.906, Seconds: 41.31\n",
      "Epoch  46/50 Batch 1500/1837 - Loss: 126.089, Seconds: 44.25\n",
      "Epoch  46/50 Batch 1800/1837 - Loss: 127.496, Seconds: 50.64\n",
      "Valid Loss: 412.470, Seconds: 11.87\n",
      "No Improvement.\n",
      "Epoch  47/50 Batch    0/1837 - Loss: 17.676, Seconds: 33.82\n",
      "Epoch  47/50 Batch  300/1837 - Loss: 151.202, Seconds: 37.05\n",
      "Epoch  47/50 Batch  600/1837 - Loss: 116.008, Seconds: 37.91\n",
      "Epoch  47/50 Batch  900/1837 - Loss: 139.900, Seconds: 39.76\n",
      "Valid Loss: 179.917, Seconds: 11.90\n",
      "No Improvement.\n",
      "Epoch  47/50 Batch 1200/1837 - Loss: 104.794, Seconds: 42.19\n",
      "Epoch  47/50 Batch 1500/1837 - Loss: 95.606, Seconds: 43.76\n",
      "Epoch  47/50 Batch 1800/1837 - Loss: 95.608, Seconds: 49.38\n",
      "Valid Loss: 281.973, Seconds: 11.91\n",
      "No Improvement.\n",
      "Epoch  48/50 Batch    0/1837 - Loss: 13.704, Seconds: 33.63\n",
      "Epoch  48/50 Batch  300/1837 - Loss: 121.175, Seconds: 37.20\n",
      "Epoch  48/50 Batch  600/1837 - Loss: 140.576, Seconds: 37.80\n",
      "Epoch  48/50 Batch  900/1837 - Loss: 101.232, Seconds: 39.27\n",
      "Valid Loss: 197.183, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  48/50 Batch 1200/1837 - Loss: 109.787, Seconds: 41.08\n",
      "Epoch  48/50 Batch 1500/1837 - Loss: 105.653, Seconds: 44.62\n",
      "Epoch  48/50 Batch 1800/1837 - Loss: 140.027, Seconds: 49.91\n",
      "Valid Loss: 269.138, Seconds: 11.91\n",
      "No Improvement.\n",
      "Epoch  49/50 Batch    0/1837 - Loss: 12.008, Seconds: 33.91\n",
      "Epoch  49/50 Batch  300/1837 - Loss: 175.082, Seconds: 37.58\n",
      "Epoch  49/50 Batch  600/1837 - Loss: 149.107, Seconds: 37.90\n",
      "Epoch  49/50 Batch  900/1837 - Loss: 120.540, Seconds: 41.05\n",
      "Valid Loss: 314.920, Seconds: 11.90\n",
      "No Improvement.\n",
      "Epoch  49/50 Batch 1200/1837 - Loss: 121.270, Seconds: 41.77\n",
      "Epoch  49/50 Batch 1500/1837 - Loss: 106.167, Seconds: 44.04\n",
      "Epoch  49/50 Batch 1800/1837 - Loss: 97.707, Seconds: 49.70\n",
      "Valid Loss: 349.947, Seconds: 11.90\n",
      "No Improvement.\n",
      "Epoch  50/50 Batch    0/1837 - Loss: 17.199, Seconds: 33.74\n",
      "Epoch  50/50 Batch  300/1837 - Loss: 129.371, Seconds: 36.93\n",
      "Epoch  50/50 Batch  600/1837 - Loss: 135.029, Seconds: 37.60\n",
      "Epoch  50/50 Batch  900/1837 - Loss: 111.482, Seconds: 39.43\n",
      "Valid Loss: 223.350, Seconds: 11.89\n",
      "No Improvement.\n",
      "Epoch  50/50 Batch 1200/1837 - Loss: 98.777, Seconds: 41.56\n",
      "Epoch  50/50 Batch 1500/1837 - Loss: 77.668, Seconds: 44.10\n",
      "Epoch  50/50 Batch 1800/1837 - Loss: 107.501, Seconds: 49.15\n",
      "Valid Loss: 225.290, Seconds: 11.88\n",
      "No Improvement.\n"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN THIS BLOCK IF YOU'RE LOADING A MODEL!! See the instruction in the previous block\n",
    "\n",
    "display_step = 300 # Check training loss after every 300 batches\n",
    "validation_check = ((len(train_questions)) // batch_size // 2) - 1\n",
    "total_train_loss = 0 # Record the training loss for each display step\n",
    "summary_valid_loss = [] # Record the validation loss for saving improvements in the model\n",
    "itr = 1 # Cur iteration\n",
    "\n",
    "checkpoint = \"./ckpts/best_model.ckpt\" \n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch_i in range(1, epochs + 1):\n",
    "    for batch_i, (questions_batch, answers_batch) in enumerate(batch_data(train_questions, train_answers, batch_size)):\n",
    "        start_time = time.time()\n",
    "        _, loss = sess.run([train_op, seq_loss],{input_data: questions_batch, targets: answers_batch, lr: learning_rate,\n",
    "                                                 sequence_length: answers_batch.shape[1], keep_prob: keep_probability})\n",
    "\n",
    "        total_train_loss += loss\n",
    "        end_time = time.time()\n",
    "        batch_time = end_time - start_time\n",
    "\n",
    "        if batch_i % display_step == 0:\n",
    "            print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'.format(epoch_i, epochs, batch_i, \n",
    "                          len(train_questions) // batch_size, total_train_loss / display_step, batch_time*display_step))\n",
    "            total_train_loss = 0\n",
    "\n",
    "        if batch_i % validation_check == 0 and batch_i > 0:\n",
    "            total_valid_loss = 0\n",
    "            start_time = time.time()\n",
    "            for batch_ii, (questions_batch, answers_batch) in enumerate(batch_data(test_questions, test_answers, batch_size)):\n",
    "                valid_loss = sess.run(seq_loss, {input_data: questions_batch, targets: answers_batch, lr: learning_rate,\n",
    "                                                 sequence_length: answers_batch.shape[1], keep_prob: 1})\n",
    "                total_valid_loss += valid_loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "            avg_valid_loss = total_valid_loss / (len(test_questions) / batch_size)\n",
    "            print('Valid Loss: {:>6.3f}, Seconds: {:>5.2f}'.format(avg_valid_loss, batch_time))\n",
    "            \n",
    "            # Reduce learning rate, but not below its minimum value\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "            \n",
    "            if itr > 1:\n",
    "                summary_valid_loss.append(avg_valid_loss)\n",
    "            if itr > 1 and avg_valid_loss <= min(summary_valid_loss): \n",
    "                print('New Record!') \n",
    "                #saver = tf.train.Saver() \n",
    "                #saver.save(sess, checkpoint)\n",
    "            \n",
    "            elif itr == 1:\n",
    "                print (\"Not Fully Initiated\")\n",
    "            \n",
    "            else:\n",
    "                print(\"No Improvement.\")\n",
    "            \n",
    "            saver = tf.train.Saver() \n",
    "            saver.save(sess, checkpoint)\n",
    "            itr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  116.  5882.  6824.  1269.  6459.   944.  1925.  3701.  6712.  8093.\n",
      "  8092.  8092.  8092.  8092.  8092.  8092.  8092.  8092.  8092.  8092.]\n",
      "Question\n",
      "  Input Words: ['that', 'was', 'so', 'good', 'i', 'am', 'gonna', 'have', 'another', '<EOS>']\n",
      "\n",
      "Answer\n",
      "  Response Words: ['you', 'is', 'a']\n"
     ]
    }
   ],
   "source": [
    "def question_to_seq(question, vocab_to_int):\n",
    "    #Prepare the question for the model\n",
    "    question = clean_text(question)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in question.split()]\n",
    "\n",
    "# Create your own input question\n",
    "input_question = 'I like you'\n",
    "\n",
    "# Use a question from the data as your input\n",
    "random = np.random.choice(len(short_questions))\n",
    "input_question = short_questions[random]\n",
    "\n",
    "# Prepare input question\n",
    "input_question = input_question\n",
    "input_question = question_to_seq(input_question, questions_vocab_to_int)\n",
    "input_question.append(questions_vocab_to_int['<EOS>'])\n",
    "\n",
    "# Pad the questions until it equals the max_line_length\n",
    "input_question = input_question + [questions_vocab_to_int[\"<PAD>\"]] * (max_line_length - len(input_question))\n",
    "# Add empty questions so the the input_data is the correct shape\n",
    "batch_shell = np.zeros((batch_size, max_line_length))\n",
    "# Set the first question to be out input question\n",
    "batch_shell[0] = input_question    \n",
    "    \n",
    "# Run the model with the input question\n",
    "answer_logits = sess.run(inference_logits, {input_data: batch_shell, keep_prob: 1.0})[0]\n",
    "\n",
    "print (batch_shell[0])\n",
    "# Remove the padding from the Question and Answer\n",
    "pad_q = questions_vocab_to_int[\"<PAD>\"]\n",
    "pad_a = answers_vocab_to_int[\"<PAD>\"]\n",
    "print('Question')\n",
    "print('  Input Words: {}'.format([questions_int_to_vocab[i] for i in input_question if i != pad_q]))\n",
    "print()\n",
    "print('Answer')\n",
    "print('  Response Words: {}'.format([answers_int_to_vocab[i] for i in np.argmax(answer_logits, 1) if i != pad_a]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
